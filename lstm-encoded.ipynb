{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8435e077",
   "metadata": {},
   "source": [
    "# LSTM Model for Cache Hit Prediction\n",
    "\n",
    "This notebook implements a Long Short-Term Memory (LSTM) neural network to predict cache hits based on memory access patterns. The model analyzes sequences of memory addresses to learn patterns that indicate whether a future memory access will result in a cache hit or miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bade637-1eb2-44c5-8360-0a9def7b3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# NumPy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch data utilities\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "# Advanced dictionary for counting and aggregation\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358f0e2",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Importing the necessary libraries:\n",
    "- `torch` and `torch.nn`: PyTorch deep learning framework\n",
    "- `numpy`: For numerical operations\n",
    "- `matplotlib.pyplot`: For plotting and visualization\n",
    "- `DataLoader` and `TensorDataset`: For batching and dataset handling\n",
    "- `defaultdict`: For advanced dictionary operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567bf7a1-662a-4961-902f-fb771c068b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the trace file containing memory access data\n",
    "with open(\"test_red.out\") as f:\n",
    "    addrs = f.readlines()\n",
    "\n",
    "# Split each line into hit/miss indicator and memory address\n",
    "# The format is: 'hit_count memory_address'\n",
    "hits, addrs = zip(*(l.rstrip().split(' ') for l in addrs))\n",
    "\n",
    "# Convert hit counts to integers\n",
    "hits = [int(hit) for hit in hits]\n",
    "\n",
    "# Convert memory addresses from hex to integers\n",
    "addrs = [int(addr, 16) for addr in addrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ab669",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Loading the cache access data from a file. Each line contains:\n",
    "- A hit/miss indicator (1 for hit, 0 for miss)\n",
    "- A memory address in hexadecimal format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3f80de-dd12-48f9-9289-d5ee26aefc36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [536870744,\n",
       "  536870752,\n",
       "  805385384,\n",
       "  805385392,\n",
       "  805385376,\n",
       "  805385400,\n",
       "  536870728,\n",
       "  536870720,\n",
       "  536870712,\n",
       "  536870704])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits[:10], addrs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c1d155-ec4d-48a2-9399-62510e530973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       "  tensor([0.])),\n",
       " (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       "  tensor([0.])))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AddrDataset(Dataset):\n",
    "    def __init__(self, addresses, hits, seq_length):\n",
    "        self.addresses = np.asarray(addresses)\n",
    "        unique_addrs = np.unique(self.addresses)\n",
    "        self.num_classes = len(unique_addrs)\n",
    "        self.addr_to_index = {addr: i for i, addr in enumerate(unique_addrs)}\n",
    "        self.hits = torch.tensor(np.asarray(hits) > 0).float()\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.addresses) - self.seq_length - 1 - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0:\n",
    "            idx = len(self) - idx\n",
    "        addrs = self.addresses[idx:idx + self.seq_length]\n",
    "        one_hot_x = torch.stack([F.one_hot(torch.tensor(self.addr_to_index[addr]), num_classes=self.num_classes).float() for addr in addrs])\n",
    "        label = self.hits[idx + self.seq_length]\n",
    "        return one_hot_x, label.unsqueeze(0)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"AddrDataset({self.num_classes=})\"\n",
    "\n",
    "dataset = AddrDataset(addrs, hits, 15)\n",
    "dataset\n",
    "(dataset[0], dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44eefb4d-da8e-4aa4-a64f-a6c8397d16dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA-compatible GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6452a8a",
   "metadata": {},
   "source": [
    "## Device Configuration\n",
    "\n",
    "Setting up the computation device - will use GPU (CUDA) if available, otherwise CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd008b5",
   "metadata": {},
   "source": [
    "## LSTM Model Definition\n",
    "\n",
    "Defining the Long Short-Term Memory (LSTM) neural network architecture:\n",
    "\n",
    "- **Input**: Sequences of normalized memory addresses\n",
    "- **LSTM Layer**: Processes sequences and captures temporal patterns\n",
    "- **Fully Connected Layer**: Maps LSTM output to binary prediction\n",
    "\n",
    "The model maintains and updates hidden state (h) and cell state (c) between batches for continuous learning on sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fab9eb9-fa9c-42a0-a7a8-227740e26758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, layer_dim, output_dim):\n",
    "        '''\n",
    "        Initialize the LSTM model.\n",
    "        \n",
    "        Parameters:\n",
    "            input_dim (int): Size of input feature dimension (number of unique addresses for one-hot encoding)\n",
    "            hidden_dim (int): Size of the hidden state\n",
    "            layer_dim (int): Number of LSTM layers\n",
    "            output_dim (int): Size of output (1 for binary prediction)\n",
    "        '''\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "\n",
    "        #self.relu = nn.ReLU()\n",
    "        \n",
    "        # LSTM layer with batch_first=True means input shape is [batch, seq, feature]\n",
    "        self.lstm = nn.LSTM(embedding_dim, output_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to produce output prediction\n",
    "        #self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        '''\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "            x (tensor): Input tensor of shape [batch_size, seq_len, input_dim]\n",
    "            h0 (tensor, optional): Initial hidden state\n",
    "            c0 (tensor, optional): Initial cell state\n",
    "            \n",
    "        Returns:\n",
    "            out (tensor): Output predictions\n",
    "            hn (tensor): Final hidden state\n",
    "            cn (tensor): Final cell state\n",
    "        '''\n",
    "        # Initialize hidden states if not provided\n",
    "        if h0 is None or c0 is None:\n",
    "            # Create zero tensors for hidden and cell states\n",
    "            # Shape: [num_layers, batch_size, hidden_dim]\n",
    "            h0 = torch.zeros(self.layer_dim, x.size(0), self.output_dim).to(x.device)\n",
    "            c0 = torch.zeros(self.layer_dim, x.size(0), self.output_dim).to(x.device)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        \n",
    "        # Forward propagate the LSTM\n",
    "        # out shape: [batch_size, seq_len, hidden_dim]\n",
    "        # hn and cn shape: [num_layers, batch_size, hidden_dim]\n",
    "        out, (hn, cn) = self.lstm(out, (h0, c0))\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        # Only take the output from the final timestep\n",
    "        #out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return self.sigmoid(out), hn, cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6d37d78-5233-4392-905d-74433c0b9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of features (dimension of one-hot encoded vectors)\n",
    "num_features = dataset.num_classes  # The length of each one-hot encoded vector\n",
    "\n",
    "# Initialize the LSTM model\n",
    "# - input_dim=num_features: Each timestep has a one-hot encoded vector as feature\n",
    "# - hidden_dim=100: Size of the hidden state vector\n",
    "# - layer_dim=1: Single LSTM layer\n",
    "# - output_dim=1: Binary output (hit probability)\n",
    "model = LSTMModel(input_dim=num_features, embedding_dim=128, hidden_dim=100, layer_dim=1, output_dim=1).to(device)\n",
    "\n",
    "# Define loss function - Mean Squared Error\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define optimizer - Adam with learning rate of 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f109714",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "Initializing the LSTM model with the following configuration:\n",
    "- Input dimension: num_features (number of unique memory addresses for one-hot encoding)\n",
    "- Hidden dimension: 100 (size of LSTM cell state)\n",
    "- Layer dimension: 1 (single LSTM layer)\n",
    "- Output dimension: 1 (binary prediction)\n",
    "\n",
    "The one-hot encoding increases the input dimension but allows the model to distinguish between individual memory addresses more effectively.\n",
    "\n",
    "Also configuring the loss function (Mean Squared Error) and optimizer (Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fb244df-c612-465c-a92e-f7bcadec0c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size for training\n",
    "# Larger batch size can speed up training but requires more memory\n",
    "batch_size = 2048\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "# - shuffle=True: Randomizes data order in each epoch\n",
    "# - drop_last=True: Drops the last incomplete batch\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ea1f3",
   "metadata": {},
   "source": [
    "## DataLoader Configuration\n",
    "\n",
    "Preparing the data for batch processing using PyTorch's DataLoader:\n",
    "- Creates a TensorDataset from input sequences and target values\n",
    "- Configures the batch size, shuffling, and other training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fc1d5c8-b56f-490c-9647-c32a0c1364aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (15) must match the size of tensor b (2048) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m outputs, h0, c0 = model(x_batch, h0, c0)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Calculate batch accuracy (threshold at 0.5 for binary classification)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m accuracy = (\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m).sum() / x_batch.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[32m     27\u001b[39m loss = criterion(outputs, y_batch)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (15) must match the size of tensor b (2048) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Number of complete passes through the dataset\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize hidden and cell states as None (will be created in first forward pass)\n",
    "h0, c0 = None, None\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    # Zero all gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Process each batch in the dataset\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Unpack inputs and targets from batch\n",
    "        x_batch, y_batch = batch\n",
    "        x_batch, y_batch = (x_batch.to(device), y_batch.to(device))\n",
    "        \n",
    "        # Forward pass: compute predictions and get new hidden states\n",
    "        outputs, h0, c0 = model(x_batch, h0, c0)\n",
    "\n",
    "        # Calculate batch accuracy (threshold at 0.5 for binary classification)\n",
    "        accuracy = ((outputs > 0.5) == y_batch).sum() / x_batch.shape[0]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model parameters based on gradients\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Detach hidden states from the computation graph to prevent \n",
    "        # backpropagation through the entire history (avoids exploding gradients)\n",
    "        h0 = h0.detach()\n",
    "        c0 = c0.detach()\n",
    "        \n",
    "        # Print progress every 10 batches\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {i}, Batch Loss: {loss.item():.4f}, Batch Accuracy {accuracy}\")\n",
    "    \n",
    "    # Alternative epoch-level reporting (commented out)\n",
    "    #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1231d874",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Training the LSTM model with the following process:\n",
    "1. Iterate through epochs\n",
    "2. For each batch in the dataloader:\n",
    "   - Forward pass through the model\n",
    "   - Calculate accuracy and loss\n",
    "   - Backward pass to compute gradients\n",
    "   - Update model parameters\n",
    "\n",
    "Note: The hidden and cell states are preserved between batches but detached from the computation graph to prevent exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f26d2a-25b5-4dc0-8753-fd1ad495e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for model evaluation (uncomment to use)\n",
    "\n",
    "# def evaluate_model(model, X_test, y_test):\n",
    "#     '''\n",
    "#     Evaluate the model on test data\n",
    "#     \n",
    "#     Parameters:\n",
    "#         model: Trained LSTM model\n",
    "#         X_test: Test input sequences with one-hot encoded addresses\n",
    "#         y_test: Test labels\n",
    "#     \n",
    "#     Returns:\n",
    "#         accuracy: Model accuracy on test data\n",
    "#     '''\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     with torch.no_grad():  # Disable gradient computation\n",
    "#         # Convert test data to tensors\n",
    "#         X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)  # No need for extra dimension with one-hot\n",
    "#         y_test_tensor = torch.tensor(y_test[:, None], dtype=torch.float32).to(device)\n",
    "#         \n",
    "#         # Forward pass\n",
    "#         outputs, _, _ = model(X_test_tensor)\n",
    "#         \n",
    "#         # Calculate accuracy\n",
    "#         predictions = (outputs > 0.5).float()\n",
    "#         accuracy = (predictions == y_test_tensor).sum() / len(y_test_tensor)\n",
    "#     \n",
    "#     return accuracy.item()\n",
    "\n",
    "# # Plot training history\n",
    "# def plot_training_history(history):\n",
    "#     '''\n",
    "#     Plot the training loss and accuracy over epochs\n",
    "#     \n",
    "#     Parameters:\n",
    "#         history: Dictionary containing 'loss' and 'accuracy' lists\n",
    "#     '''\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "#     \n",
    "#     # Plot loss\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(history['loss'])\n",
    "#     plt.title('Training Loss')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     \n",
    "#     # Plot accuracy\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(history['accuracy'])\n",
    "#     plt.title('Training Accuracy')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Function to visualize the one-hot encoded data\n",
    "# def visualize_onehot_data(X, num_samples=5):\n",
    "#     '''\n",
    "#     Visualizes a few examples of one-hot encoded memory addresses\n",
    "#     \n",
    "#     Parameters:\n",
    "#         X: One-hot encoded data\n",
    "#         num_samples: Number of samples to visualize\n",
    "#     '''\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     for i in range(min(num_samples, len(X))):\n",
    "#         plt.subplot(num_samples, 1, i+1)\n",
    "#         plt.imshow(X[i].reshape(1, -1), aspect='auto', cmap='viridis')\n",
    "#         plt.title(f\"Sample {i+1}\")\n",
    "#         plt.ylabel(\"One-hot vector\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58d5b6",
   "metadata": {},
   "source": [
    "## Model Evaluation and Further Steps\n",
    "\n",
    "After training, the model can be used to predict cache hits for new memory address sequences. Possible next steps:\n",
    "\n",
    "1. **Model Evaluation**: Test the model on a separate validation set to assess generalization performance\n",
    "2. **Hyperparameter Tuning**: Experiment with different LSTM configurations (hidden size, number of layers)\n",
    "3. **Feature Engineering**: Consider additional features like program counter values\n",
    "4. **Visualization**: Plot the training loss and accuracy curves\n",
    "5. **Inference**: Use the trained model to predict cache behavior for new memory traces\n",
    "6. **Encoding Optimization**: If the number of unique addresses is very large, consider dimensionality reduction techniques or address clustering to reduce the one-hot vector size\n",
    "7. **Memory Efficiency**: For extremely large address spaces, explore memory-efficient alternatives to one-hot encoding, such as embedding layers that learn dense representations of addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b3d98",
   "metadata": {},
   "source": [
    "## Memory Considerations for One-Hot Encoding\n",
    "\n",
    "One-hot encoding provides clear separation between different memory addresses, which can improve model learning. However, it comes with memory trade-offs:\n",
    "\n",
    "**Advantages:**\n",
    "- Treats each address as a distinct entity\n",
    "- Avoids imposing an artificial numerical relationship between addresses\n",
    "- May capture patterns that normalized values would miss\n",
    "\n",
    "**Challenges:**\n",
    "- High dimensionality with large address spaces (potentially millions of unique addresses)\n",
    "- Sparse representation (mostly zeros) requiring more memory\n",
    "- May need dimensionality reduction for very large address spaces\n",
    "\n",
    "**Possible Solutions:**\n",
    "- Address bucketing: Group similar addresses into buckets\n",
    "- Page-level encoding: Encode at page granularity instead of exact addresses\n",
    "- Embedding layers: Learn dense representations of addresses\n",
    "- Feature hashing: Map addresses to a smaller feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668713a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of more memory-efficient alternatives to full one-hot encoding\n",
    "# Uncomment and modify as needed\n",
    "\n",
    "# Option 1: Address bucketing - group addresses into a smaller number of buckets\n",
    "# def create_address_buckets(addresses, num_buckets=1000):\n",
    "#     min_addr = min(addresses)\n",
    "#     max_addr = max(addresses)\n",
    "#     bucket_size = (max_addr - min_addr) / num_buckets\n",
    "#     \n",
    "#     # Assign each address to a bucket\n",
    "#     buckets = []\n",
    "#     for addr in addresses:\n",
    "#         bucket = int((addr - min_addr) / bucket_size)\n",
    "#         buckets.append(min(bucket, num_buckets-1))  # Cap at max bucket\n",
    "#         \n",
    "#     # One-hot encode the buckets instead of raw addresses\n",
    "#     bucket_onehot = np.zeros((len(buckets), num_buckets))\n",
    "#     for i, bucket in enumerate(buckets):\n",
    "#         bucket_onehot[i, bucket] = 1\n",
    "#         \n",
    "#     return bucket_onehot\n",
    "\n",
    "# Option 2: Embedding layer approach - learn dense vector representations\n",
    "# class EmbeddingLSTMModel(nn.Module):\n",
    "#     def __init__(self, num_addresses, embedding_dim, hidden_dim, layer_dim, output_dim):\n",
    "#         super(EmbeddingLSTMModel, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.layer_dim = layer_dim\n",
    "#         \n",
    "#         # Embedding layer converts address indices to dense vectors\n",
    "#         self.embedding = nn.Embedding(num_addresses, embedding_dim)\n",
    "#         \n",
    "#         # LSTM layer\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "#         \n",
    "#         # Fully connected layer\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "#     \n",
    "#     def forward(self, x, h0=None, c0=None):\n",
    "#         # Convert address indices to embeddings\n",
    "#         # x shape: [batch_size, seq_len] -> [batch_size, seq_len, embedding_dim]\n",
    "#         embedded = self.embedding(x)\n",
    "#         \n",
    "#         # Initialize hidden states if not provided\n",
    "#         if h0 is None or c0 is None:\n",
    "#             h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "#             c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "#         \n",
    "#         # Forward propagate LSTM\n",
    "#         out, (hn, cn) = self.lstm(embedded, (h0, c0))\n",
    "#         \n",
    "#         # Decode the hidden state of the last time step\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         \n",
    "#         return out, hn, cn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
