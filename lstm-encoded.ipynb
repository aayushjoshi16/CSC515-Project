{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8435e077",
   "metadata": {},
   "source": [
    "# LSTM Model for Cache Hit Prediction\n",
    "\n",
    "This notebook implements a Long Short-Term Memory (LSTM) neural network to predict cache hits based on memory access patterns. The model analyzes sequences of memory addresses to learn patterns that indicate whether a future memory access will result in a cache hit or miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bade637-1eb2-44c5-8360-0a9def7b3d38",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'autocast' from 'torch.amp' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# PyTorch libraries for deep learning\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# NumPy for numerical operations\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aayus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:2043\u001b[39m\n\u001b[32m   2041\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m amp \u001b[38;5;28;01mas\u001b[39;00m amp, random \u001b[38;5;28;01mas\u001b[39;00m random, serialization \u001b[38;5;28;01mas\u001b[39;00m serialization\n\u001b[32m   2042\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_tensor_str\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_printoptions\n\u001b[32m-> \u001b[39m\u001b[32m2043\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mamp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m autocast, GradScaler\n\u001b[32m   2044\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrandom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_rng_state, initial_seed, manual_seed, seed, set_rng_state\n\u001b[32m   2045\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mserialization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load, save\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'autocast' from 'torch.amp' (unknown location)"
     ]
    }
   ],
   "source": [
    "# PyTorch libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# NumPy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch data utilities\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Advanced dictionary for counting and aggregation\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358f0e2",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Importing the necessary libraries:\n",
    "- `torch` and `torch.nn`: PyTorch deep learning framework\n",
    "- `numpy`: For numerical operations\n",
    "- `matplotlib.pyplot`: For plotting and visualization\n",
    "- `DataLoader` and `TensorDataset`: For batching and dataset handling\n",
    "- `defaultdict`: For advanced dictionary operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "567bf7a1-662a-4961-902f-fb771c068b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the trace file containing memory access data\n",
    "with open(\"test1.out\") as f:\n",
    "    addrs = f.readlines()\n",
    "\n",
    "# Split each line into hit/miss indicator and memory address\n",
    "# The format is: 'hit_count memory_address'\n",
    "hits, addrs = zip(*(l.rstrip().split(' ') for l in addrs))\n",
    "\n",
    "# Convert hit counts to integers\n",
    "hits = [int(hit) for hit in hits]\n",
    "\n",
    "# Convert memory addresses from hex to integers\n",
    "addrs = [int(addr, 16) for addr in addrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ab669",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Loading the cache access data from a file. Each line contains:\n",
    "- A hit/miss indicator (1 for hit, 0 for miss)\n",
    "- A memory address in hexadecimal format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f3f80de-dd12-48f9-9289-d5ee26aefc36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [536870744,\n",
       "  536870752,\n",
       "  805385384,\n",
       "  805385392,\n",
       "  805385376,\n",
       "  805385400,\n",
       "  536870728,\n",
       "  536870720,\n",
       "  536870712,\n",
       "  536870704])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits[:10], addrs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d421f233-803e-4a40-be26-9983478561c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 286982 unique memory addresses\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.19 MiB for an array with shape (286982,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Create one-hot encoding for each memory address\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m addr \u001b[38;5;129;01min\u001b[39;00m unique_addrs:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     one_hot = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munique_addrs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     one_hot[addr_to_index[addr]] = \u001b[32m1\u001b[39m\n\u001b[32m     19\u001b[39m     addr_to_onehot[addr] = one_hot\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 2.19 MiB for an array with shape (286982,) and data type float64"
     ]
    }
   ],
   "source": [
    "# Convert hit counts to binary target values\n",
    "# Any positive hit count becomes 1, 0 remains 0\n",
    "y = np.asarray(hits)\n",
    "y = (y > 0).astype(np.int_)  # 1 for hit, 0 for miss\n",
    "\n",
    "# Create a one-hot encoding\n",
    "# Create a mapping for memory addresses\n",
    "unique_addrs = sorted(list(set(addrs)))\n",
    "addr_to_index = {addr: i for i, addr in enumerate(unique_addrs)}\n",
    "print(f\"Found {len(unique_addrs)} unique memory addresses\")\n",
    "\n",
    "# Create a dictionary to store one-hot vectors for each memory address\n",
    "addr_to_onehot = {}\n",
    "\n",
    "# Create one-hot encoding for each memory address\n",
    "for addr in unique_addrs:\n",
    "    one_hot = np.zeros(len(unique_addrs))\n",
    "    one_hot[addr_to_index[addr]] = 1\n",
    "    addr_to_onehot[addr] = one_hot\n",
    "\n",
    "# Map original addresses to their one-hot encoding\n",
    "X = np.array([addr_to_onehot[addr] for addr in addrs])\n",
    "\n",
    "# Check dimensions of the one-hot encoded data\n",
    "print(f\"One-hot encoded shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc4e56e",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Preprocessing the data for model training:\n",
    "1. Converting hit counts to binary values (0 = miss, 1 = hit)\n",
    "2. Using one-hot encoding for memory addresses instead of normalization\n",
    "\n",
    "One-hot encoding creates a binary vector for each memory address where:\n",
    "- The vector length equals the number of unique addresses\n",
    "- Each vector contains all 0's except for a single 1 at the position corresponding to that address\n",
    "\n",
    "This approach allows the model to treat each memory address as a distinct entity rather than a relative numeric value, which may improve the model's ability to learn patterns specific to individual addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e0a182-079c-406a-bdc7-70fadf3ea5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.05364059, -1.05364053,  0.94539084,  0.9453909 ,  0.94539079,\n",
       "        0.94539096, -1.05364071, -1.05364077, -1.05364083, -1.05364089])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1272f911-3a35-4584-a456-3300764c08c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f4a5ac6-df8b-4558-9bc3-75d3aafd1ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09269b9",
   "metadata": {},
   "source": [
    "## Sequence Generation\n",
    "\n",
    "The LSTM model requires sequential data. This function builds sequences of memory addresses (X) with their corresponding future cache hit/miss outcome (y).\n",
    "\n",
    "For each sequence:\n",
    "- Input: n consecutive memory addresses\n",
    "- Target: Whether the n+1 memory access results in a hit or miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce41eb6-ef02-42b8-a79f-92a8e31620a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seqs(X, y, n):\n",
    "    '''\n",
    "    Builds sequences from the dataset for LSTM training.\n",
    "    \n",
    "    Parameters:\n",
    "        X (array): One-hot encoded memory addresses\n",
    "        y (array): Hit/miss indicators (1/0)\n",
    "        n (int): Length of input sequence (window size)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (xs, ys) where:\n",
    "            - xs is an array of input sequences of length n\n",
    "            - ys is an array of corresponding target values\n",
    "    '''\n",
    "    assert len(X) == len(y)\n",
    "    xs = []  # Will hold input sequences\n",
    "    ys = []  # Will hold target values\n",
    "    \n",
    "    # Create sliding windows of size n\n",
    "    for i in range(len(X)-n):\n",
    "        # Extract a sequence of n consecutive addresses\n",
    "        x_sample = X[i:(i+n)]\n",
    "        # Target is whether the next address (after the sequence) results in a hit\n",
    "        y_sample = y[i+n]\n",
    "        xs.append(x_sample)\n",
    "        ys.append(y_sample)\n",
    "        \n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55251ca1-610e-4983-a67a-63d1feea60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequences with a window size of 15 addresses\n",
    "# Each sequence contains 15 normalized memory addresses\n",
    "# The target is whether the 16th access is a hit (1) or miss (0)\n",
    "Xs, ys = build_seqs(X, y, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ce015dc-cf50-47f9-b9cb-738614eb3aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.05364059, -1.05364053,  0.94539084,  0.9453909 ,  0.94539079,\n",
       "        0.94539096, -1.05364071, -1.05364077, -1.05364083, -1.05364089,\n",
       "        0.9453859 , -1.05364089,  0.94514165,  0.94514207,  0.94514249])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e4c9b98-a1e8-4304-be66-83f51872aff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44eefb4d-da8e-4aa4-a64f-a6c8397d16dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA-compatible GPU is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6452a8a",
   "metadata": {},
   "source": [
    "## Device Configuration\n",
    "\n",
    "Setting up the computation device - will use GPU (CUDA) if available, otherwise CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303ca5b1-5ae7-49a1-b459-51b0be10c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input sequences (X) to PyTorch tensors\n",
    "# Shape becomes [num_samples, sequence_length, feature_dim]\n",
    "trainX = torch.tensor(Xs, dtype=torch.float32).to(device)\n",
    "\n",
    "# Convert target values (y) to PyTorch tensors\n",
    "# Shape becomes [num_samples, 1]\n",
    "trainY = torch.tensor(ys[:, None], dtype=torch.float32).to(device)\n",
    "\n",
    "# Print the shape of the training tensors\n",
    "print(f\"Training input shape: {trainX.shape}\")\n",
    "print(f\"Training target shape: {trainY.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a35b08",
   "metadata": {},
   "source": [
    "## Tensor Conversion\n",
    "\n",
    "Converting NumPy arrays to PyTorch tensors, which are required for model training:\n",
    "- Each timestep has a feature vector\n",
    "- Moving tensors to the selected device (GPU/CPU)\n",
    "- Setting the appropriate data type (float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b7469b3-ba66-49d4-969e-75012db4fe85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0536],\n",
       "         [-1.0536],\n",
       "         [ 0.9454],\n",
       "         ...,\n",
       "         [ 0.9451],\n",
       "         [ 0.9451],\n",
       "         [ 0.9451]],\n",
       "\n",
       "        [[-1.0536],\n",
       "         [ 0.9454],\n",
       "         [ 0.9454],\n",
       "         ...,\n",
       "         [ 0.9451],\n",
       "         [ 0.9451],\n",
       "         [-1.0536]],\n",
       "\n",
       "        [[ 0.9454],\n",
       "         [ 0.9454],\n",
       "         [ 0.9454],\n",
       "         ...,\n",
       "         [ 0.9451],\n",
       "         [-1.0536],\n",
       "         [-1.0536]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.0537],\n",
       "         [ 0.9454],\n",
       "         [-1.0537],\n",
       "         ...,\n",
       "         [-1.0537],\n",
       "         [-1.0537],\n",
       "         [-1.0537]],\n",
       "\n",
       "        [[ 0.9454],\n",
       "         [-1.0537],\n",
       "         [ 0.9450],\n",
       "         ...,\n",
       "         [-1.0537],\n",
       "         [-1.0537],\n",
       "         [-1.0537]],\n",
       "\n",
       "        [[-1.0537],\n",
       "         [ 0.9450],\n",
       "         [ 0.9450],\n",
       "         ...,\n",
       "         [-1.0537],\n",
       "         [-1.0537],\n",
       "         [ 0.9454]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d9a8db0-28b9-436a-a4ab-9a0beea66c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb329f20-4bc4-4377-bcbb-f9a958c4fa77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4999985, 15, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd008b5",
   "metadata": {},
   "source": [
    "## LSTM Model Definition\n",
    "\n",
    "Defining the Long Short-Term Memory (LSTM) neural network architecture:\n",
    "\n",
    "- **Input**: Sequences of normalized memory addresses\n",
    "- **LSTM Layer**: Processes sequences and captures temporal patterns\n",
    "- **Fully Connected Layer**: Maps LSTM output to binary prediction\n",
    "\n",
    "The model maintains and updates hidden state (h) and cell state (c) between batches for continuous learning on sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fab9eb9-fa9c-42a0-a7a8-227740e26758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        '''\n",
    "        Initialize the LSTM model.\n",
    "        \n",
    "        Parameters:\n",
    "            input_dim (int): Size of input feature dimension (number of unique addresses for one-hot encoding)\n",
    "            hidden_dim (int): Size of the hidden state\n",
    "            layer_dim (int): Number of LSTM layers\n",
    "            output_dim (int): Size of output (1 for binary prediction)\n",
    "        '''\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # LSTM layer with batch_first=True means input shape is [batch, seq, feature]\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to produce output prediction\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        '''\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "            x (tensor): Input tensor of shape [batch_size, seq_len, input_dim]\n",
    "            h0 (tensor, optional): Initial hidden state\n",
    "            c0 (tensor, optional): Initial cell state\n",
    "            \n",
    "        Returns:\n",
    "            out (tensor): Output predictions\n",
    "            hn (tensor): Final hidden state\n",
    "            cn (tensor): Final cell state\n",
    "        '''\n",
    "        # Initialize hidden states if not provided\n",
    "        if h0 is None or c0 is None:\n",
    "            # Create zero tensors for hidden and cell states\n",
    "            # Shape: [num_layers, batch_size, hidden_dim]\n",
    "            h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "            c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # Forward propagate the LSTM\n",
    "        # out shape: [batch_size, seq_len, hidden_dim]\n",
    "        # hn and cn shape: [num_layers, batch_size, hidden_dim]\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        # Only take the output from the final timestep\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out, hn, cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d37d78-5233-4392-905d-74433c0b9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of features (dimension of one-hot encoded vectors)\n",
    "num_features = X.shape[1]  # The length of each one-hot encoded vector\n",
    "\n",
    "# Initialize the LSTM model\n",
    "# - input_dim=num_features: Each timestep has a one-hot encoded vector as feature\n",
    "# - hidden_dim=100: Size of the hidden state vector\n",
    "# - layer_dim=1: Single LSTM layer\n",
    "# - output_dim=1: Binary output (hit probability)\n",
    "model = LSTMModel(input_dim=num_features, hidden_dim=100, layer_dim=1, output_dim=1).to(device)\n",
    "\n",
    "# Define loss function - Mean Squared Error\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define optimizer - Adam with learning rate of 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f109714",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "Initializing the LSTM model with the following configuration:\n",
    "- Input dimension: num_features (number of unique memory addresses for one-hot encoding)\n",
    "- Hidden dimension: 100 (size of LSTM cell state)\n",
    "- Layer dimension: 1 (single LSTM layer)\n",
    "- Output dimension: 1 (binary prediction)\n",
    "\n",
    "The one-hot encoding increases the input dimension but allows the model to distinguish between individual memory addresses more effectively.\n",
    "\n",
    "Also configuring the loss function (Mean Squared Error) and optimizer (Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fb244df-c612-465c-a92e-f7bcadec0c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size for training\n",
    "# Larger batch size can speed up training but requires more memory\n",
    "batch_size = 16384\n",
    "\n",
    "# Create PyTorch Dataset from our tensors\n",
    "dataset = TensorDataset(trainX, trainY)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "# - shuffle=True: Randomizes data order in each epoch\n",
    "# - drop_last=True: Drops the last incomplete batch\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ea1f3",
   "metadata": {},
   "source": [
    "## DataLoader Configuration\n",
    "\n",
    "Preparing the data for batch processing using PyTorch's DataLoader:\n",
    "- Creates a TensorDataset from input sequences and target values\n",
    "- Configures the batch size, shuffling, and other training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fc1d5c8-b56f-490c-9647-c32a0c1364aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Batch Loss: 1.2068, Batch Accuracy 0.05206298828125\n",
      "Epoch 0, Batch 10, Batch Loss: 0.2983, Batch Accuracy 0.43463134765625\n",
      "Epoch 0, Batch 20, Batch Loss: 0.3685, Batch Accuracy 0.94818115234375\n",
      "Epoch 0, Batch 30, Batch Loss: 0.5324, Batch Accuracy 0.05157470703125\n",
      "Epoch 0, Batch 40, Batch Loss: 0.0974, Batch Accuracy 0.93701171875\n",
      "Epoch 0, Batch 50, Batch Loss: 0.2581, Batch Accuracy 0.94390869140625\n",
      "Epoch 0, Batch 60, Batch Loss: 0.1789, Batch Accuracy 0.94586181640625\n",
      "Epoch 0, Batch 70, Batch Loss: 0.0645, Batch Accuracy 0.94384765625\n",
      "Epoch 0, Batch 80, Batch Loss: 0.1186, Batch Accuracy 0.94622802734375\n",
      "Epoch 0, Batch 90, Batch Loss: 0.1530, Batch Accuracy 0.946533203125\n",
      "Epoch 0, Batch 100, Batch Loss: 0.0930, Batch Accuracy 0.9456787109375\n",
      "Epoch 0, Batch 110, Batch Loss: 0.0557, Batch Accuracy 0.94659423828125\n",
      "Epoch 0, Batch 120, Batch Loss: 0.1236, Batch Accuracy 0.94683837890625\n",
      "Epoch 0, Batch 130, Batch Loss: 0.1990, Batch Accuracy 0.9476318359375\n",
      "Epoch 0, Batch 140, Batch Loss: 0.2370, Batch Accuracy 0.94732666015625\n",
      "Epoch 0, Batch 150, Batch Loss: 0.2307, Batch Accuracy 0.9476318359375\n",
      "Epoch 0, Batch 160, Batch Loss: 0.1789, Batch Accuracy 0.947998046875\n",
      "Epoch 0, Batch 170, Batch Loss: 0.0994, Batch Accuracy 0.9500732421875\n",
      "Epoch 0, Batch 180, Batch Loss: 0.0519, Batch Accuracy 0.94647216796875\n",
      "Epoch 0, Batch 190, Batch Loss: 0.0849, Batch Accuracy 0.94573974609375\n",
      "Epoch 0, Batch 200, Batch Loss: 0.2201, Batch Accuracy 0.50689697265625\n",
      "Epoch 0, Batch 210, Batch Loss: 0.3241, Batch Accuracy 0.13043212890625\n",
      "Epoch 0, Batch 220, Batch Loss: 0.3171, Batch Accuracy 0.05511474609375\n",
      "Epoch 0, Batch 230, Batch Loss: 0.2453, Batch Accuracy 0.55859375\n",
      "Epoch 0, Batch 240, Batch Loss: 0.1420, Batch Accuracy 0.9493408203125\n",
      "Epoch 0, Batch 250, Batch Loss: 0.0647, Batch Accuracy 0.9495849609375\n",
      "Epoch 0, Batch 260, Batch Loss: 0.0556, Batch Accuracy 0.94635009765625\n",
      "Epoch 0, Batch 270, Batch Loss: 0.1142, Batch Accuracy 0.94635009765625\n",
      "Epoch 0, Batch 280, Batch Loss: 0.2254, Batch Accuracy 0.94366455078125\n",
      "Epoch 0, Batch 290, Batch Loss: 0.3392, Batch Accuracy 0.94525146484375\n",
      "Epoch 0, Batch 300, Batch Loss: 0.4154, Batch Accuracy 0.946533203125\n",
      "Epoch 1, Batch 0, Batch Loss: 0.4202, Batch Accuracy 0.94989013671875\n",
      "Epoch 1, Batch 10, Batch Loss: 0.4109, Batch Accuracy 0.94769287109375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m loss = criterion(outputs, y_batch)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Backward pass: compute gradients\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Update model parameters based on gradients\u001b[39;00m\n\u001b[32m     32\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aayus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aayus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aayus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Number of complete passes through the dataset\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize hidden and cell states as None (will be created in first forward pass)\n",
    "h0, c0 = None, None\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    # Zero all gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Process each batch in the dataset\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Unpack inputs and targets from batch\n",
    "        x_batch, y_batch = batch\n",
    "        \n",
    "        # Forward pass: compute predictions and get new hidden states\n",
    "        outputs, h0, c0 = model(x_batch, h0, c0)\n",
    "\n",
    "        # Calculate batch accuracy (threshold at 0.5 for binary classification)\n",
    "        accuracy = ((outputs > 0.5) == y_batch).sum() / x_batch.shape[0]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model parameters based on gradients\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Detach hidden states from the computation graph to prevent \n",
    "        # backpropagation through the entire history (avoids exploding gradients)\n",
    "        h0 = h0.detach()\n",
    "        c0 = c0.detach()\n",
    "        \n",
    "        # Print progress every 10 batches\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {i}, Batch Loss: {loss.item():.4f}, Batch Accuracy {accuracy}\")\n",
    "    \n",
    "    # Alternative epoch-level reporting (commented out)\n",
    "    #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1231d874",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Training the LSTM model with the following process:\n",
    "1. Iterate through epochs\n",
    "2. For each batch in the dataloader:\n",
    "   - Forward pass through the model\n",
    "   - Calculate accuracy and loss\n",
    "   - Backward pass to compute gradients\n",
    "   - Update model parameters\n",
    "\n",
    "Note: The hidden and cell states are preserved between batches but detached from the computation graph to prevent exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f26d2a-25b5-4dc0-8753-fd1ad495e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for model evaluation (uncomment to use)\n",
    "\n",
    "# def evaluate_model(model, X_test, y_test):\n",
    "#     '''\n",
    "#     Evaluate the model on test data\n",
    "#     \n",
    "#     Parameters:\n",
    "#         model: Trained LSTM model\n",
    "#         X_test: Test input sequences with one-hot encoded addresses\n",
    "#         y_test: Test labels\n",
    "#     \n",
    "#     Returns:\n",
    "#         accuracy: Model accuracy on test data\n",
    "#     '''\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     with torch.no_grad():  # Disable gradient computation\n",
    "#         # Convert test data to tensors\n",
    "#         X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)  # No need for extra dimension with one-hot\n",
    "#         y_test_tensor = torch.tensor(y_test[:, None], dtype=torch.float32).to(device)\n",
    "#         \n",
    "#         # Forward pass\n",
    "#         outputs, _, _ = model(X_test_tensor)\n",
    "#         \n",
    "#         # Calculate accuracy\n",
    "#         predictions = (outputs > 0.5).float()\n",
    "#         accuracy = (predictions == y_test_tensor).sum() / len(y_test_tensor)\n",
    "#     \n",
    "#     return accuracy.item()\n",
    "\n",
    "# # Plot training history\n",
    "# def plot_training_history(history):\n",
    "#     '''\n",
    "#     Plot the training loss and accuracy over epochs\n",
    "#     \n",
    "#     Parameters:\n",
    "#         history: Dictionary containing 'loss' and 'accuracy' lists\n",
    "#     '''\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "#     \n",
    "#     # Plot loss\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(history['loss'])\n",
    "#     plt.title('Training Loss')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     \n",
    "#     # Plot accuracy\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(history['accuracy'])\n",
    "#     plt.title('Training Accuracy')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Function to visualize the one-hot encoded data\n",
    "# def visualize_onehot_data(X, num_samples=5):\n",
    "#     '''\n",
    "#     Visualizes a few examples of one-hot encoded memory addresses\n",
    "#     \n",
    "#     Parameters:\n",
    "#         X: One-hot encoded data\n",
    "#         num_samples: Number of samples to visualize\n",
    "#     '''\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     for i in range(min(num_samples, len(X))):\n",
    "#         plt.subplot(num_samples, 1, i+1)\n",
    "#         plt.imshow(X[i].reshape(1, -1), aspect='auto', cmap='viridis')\n",
    "#         plt.title(f\"Sample {i+1}\")\n",
    "#         plt.ylabel(\"One-hot vector\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58d5b6",
   "metadata": {},
   "source": [
    "## Model Evaluation and Further Steps\n",
    "\n",
    "After training, the model can be used to predict cache hits for new memory address sequences. Possible next steps:\n",
    "\n",
    "1. **Model Evaluation**: Test the model on a separate validation set to assess generalization performance\n",
    "2. **Hyperparameter Tuning**: Experiment with different LSTM configurations (hidden size, number of layers)\n",
    "3. **Feature Engineering**: Consider additional features like program counter values\n",
    "4. **Visualization**: Plot the training loss and accuracy curves\n",
    "5. **Inference**: Use the trained model to predict cache behavior for new memory traces\n",
    "6. **Encoding Optimization**: If the number of unique addresses is very large, consider dimensionality reduction techniques or address clustering to reduce the one-hot vector size\n",
    "7. **Memory Efficiency**: For extremely large address spaces, explore memory-efficient alternatives to one-hot encoding, such as embedding layers that learn dense representations of addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b3d98",
   "metadata": {},
   "source": [
    "## Memory Considerations for One-Hot Encoding\n",
    "\n",
    "One-hot encoding provides clear separation between different memory addresses, which can improve model learning. However, it comes with memory trade-offs:\n",
    "\n",
    "**Advantages:**\n",
    "- Treats each address as a distinct entity\n",
    "- Avoids imposing an artificial numerical relationship between addresses\n",
    "- May capture patterns that normalized values would miss\n",
    "\n",
    "**Challenges:**\n",
    "- High dimensionality with large address spaces (potentially millions of unique addresses)\n",
    "- Sparse representation (mostly zeros) requiring more memory\n",
    "- May need dimensionality reduction for very large address spaces\n",
    "\n",
    "**Possible Solutions:**\n",
    "- Address bucketing: Group similar addresses into buckets\n",
    "- Page-level encoding: Encode at page granularity instead of exact addresses\n",
    "- Embedding layers: Learn dense representations of addresses\n",
    "- Feature hashing: Map addresses to a smaller feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668713a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of more memory-efficient alternatives to full one-hot encoding\n",
    "# Uncomment and modify as needed\n",
    "\n",
    "# Option 1: Address bucketing - group addresses into a smaller number of buckets\n",
    "# def create_address_buckets(addresses, num_buckets=1000):\n",
    "#     min_addr = min(addresses)\n",
    "#     max_addr = max(addresses)\n",
    "#     bucket_size = (max_addr - min_addr) / num_buckets\n",
    "#     \n",
    "#     # Assign each address to a bucket\n",
    "#     buckets = []\n",
    "#     for addr in addresses:\n",
    "#         bucket = int((addr - min_addr) / bucket_size)\n",
    "#         buckets.append(min(bucket, num_buckets-1))  # Cap at max bucket\n",
    "#         \n",
    "#     # One-hot encode the buckets instead of raw addresses\n",
    "#     bucket_onehot = np.zeros((len(buckets), num_buckets))\n",
    "#     for i, bucket in enumerate(buckets):\n",
    "#         bucket_onehot[i, bucket] = 1\n",
    "#         \n",
    "#     return bucket_onehot\n",
    "\n",
    "# Option 2: Embedding layer approach - learn dense vector representations\n",
    "# class EmbeddingLSTMModel(nn.Module):\n",
    "#     def __init__(self, num_addresses, embedding_dim, hidden_dim, layer_dim, output_dim):\n",
    "#         super(EmbeddingLSTMModel, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.layer_dim = layer_dim\n",
    "#         \n",
    "#         # Embedding layer converts address indices to dense vectors\n",
    "#         self.embedding = nn.Embedding(num_addresses, embedding_dim)\n",
    "#         \n",
    "#         # LSTM layer\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "#         \n",
    "#         # Fully connected layer\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "#     \n",
    "#     def forward(self, x, h0=None, c0=None):\n",
    "#         # Convert address indices to embeddings\n",
    "#         # x shape: [batch_size, seq_len] -> [batch_size, seq_len, embedding_dim]\n",
    "#         embedded = self.embedding(x)\n",
    "#         \n",
    "#         # Initialize hidden states if not provided\n",
    "#         if h0 is None or c0 is None:\n",
    "#             h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "#             c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
    "#         \n",
    "#         # Forward propagate LSTM\n",
    "#         out, (hn, cn) = self.lstm(embedded, (h0, c0))\n",
    "#         \n",
    "#         # Decode the hidden state of the last time step\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         \n",
    "#         return out, hn, cn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
